{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bz2\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.models import save_model\n",
        "\n",
        "# Function to load and extract labels and texts from the file\n",
        "def load_extract(file, max_samples=None):\n",
        "    texts, labels = [], []\n",
        "    total_samples = 0\n",
        "    for line in file:\n",
        "        x = line.decode('utf-8')  # decode binary to string\n",
        "        labels.append(int(x[9]) - 1)  # extract labels\n",
        "        texts.append(x[10:].strip())  # extract texts\n",
        "        total_samples += 1\n",
        "        # Break loop if maximum number of samples is reached\n",
        "        if max_samples is not None and total_samples >= max_samples:\n",
        "            break\n",
        "    print('Done !')\n",
        "    return np.array(labels), texts\n",
        "\n",
        "# Function to clean texts\n",
        "def clean_texts(texts):\n",
        "    stwords = stopwords.words('english')\n",
        "    temp_texts = []\n",
        "    total_samples = len(texts)\n",
        "    for i, text in enumerate(texts):\n",
        "        text = re.sub('\\d','0',text) #replace every digit with 0\n",
        "        if 'www.' in text or 'http:' in text or 'https:' in text or '.com' in text: # remove links and urls\n",
        "            text = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \" \", text)\n",
        "\n",
        "        text = re.sub('[^a-zA-Z]', ' ', text) #anything which is not a character replace with whitespace char\n",
        "        text = text.lower()\n",
        "        text = text.split()\n",
        "        text = [word for word in text if not word in stwords] # remove stopwords\n",
        "        text = ' '.join(text)\n",
        "        temp_texts.append(text)\n",
        "        # Print progress every 10000 samples\n",
        "        if (i + 1) % 10000 == 0 or (i + 1) == total_samples:\n",
        "            print(f\"--{((i + 1) / total_samples) * 100:.2f}%--Done !\")\n",
        "    return temp_texts\n",
        "\n",
        "# Open the bz2 files and load data\n",
        "max_train_samples = 10000  # Set maximum number of train samples\n",
        "max_test_samples = 5000  # Set maximum number of test samples\n",
        "with bz2.BZ2File('train.ft.txt.bz2', 'r') as train_file, bz2.BZ2File('test.ft.txt.bz2', 'r') as test_file:\n",
        "    train_labels, train_texts = load_extract(train_file, max_samples=max_train_samples)\n",
        "    test_labels, test_texts = load_extract(test_file, max_samples=max_test_samples)\n",
        "\n",
        "# Cleaning the texts\n",
        "train_texts_cleaned = clean_texts(train_texts)\n",
        "test_texts_cleaned = clean_texts(test_texts)\n",
        "\n",
        "# Preprocessing\n",
        "max_words = 10000  # Max number of words to keep\n",
        "maxlen = 100  # Max length of sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts_cleaned)\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts_cleaned), maxlen=maxlen)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_texts_cleaned), maxlen=maxlen)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "# Model architecture\n",
        "embedding_dim = 100\n",
        "filters = 128\n",
        "kernel_size = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluation\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save('cnn_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LfRkkzt6WCm",
        "outputId": "2bda0271-cb26-410f-bb69-71dfedea3655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done !\n",
            "Done !\n",
            "--100.00%--Done !\n",
            "--100.00%--Done !\n",
            "Epoch 1/10\n",
            "313/313 [==============================] - 29s 87ms/step - loss: 0.4770 - accuracy: 0.7535 - val_loss: 0.3683 - val_accuracy: 0.8412\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 27s 86ms/step - loss: 0.2096 - accuracy: 0.9248 - val_loss: 0.4679 - val_accuracy: 0.8246\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 26s 82ms/step - loss: 0.0852 - accuracy: 0.9712 - val_loss: 0.6049 - val_accuracy: 0.8212\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 26s 82ms/step - loss: 0.0375 - accuracy: 0.9866 - val_loss: 0.8097 - val_accuracy: 0.8198\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 26s 83ms/step - loss: 0.0276 - accuracy: 0.9909 - val_loss: 0.9309 - val_accuracy: 0.8026\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 28s 88ms/step - loss: 0.0254 - accuracy: 0.9922 - val_loss: 1.1900 - val_accuracy: 0.8194\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 1.1838 - val_accuracy: 0.8112\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 1.5509 - val_accuracy: 0.8246\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 27s 86ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 1.6136 - val_accuracy: 0.8074\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.0142 - accuracy: 0.9955 - val_loss: 1.4320 - val_accuracy: 0.8102\n",
            "157/157 [==============================] - 3s 17ms/step - loss: 1.4320 - accuracy: 0.8102\n",
            "Test Accuracy: 0.8101999759674072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict probabilities on test data\n",
        "probabilities = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class labels based on a threshold\n",
        "predictions = (probabilities > 0.5).astype(int)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, predictions)\n",
        "\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUx7I51nvpdC",
        "outputId": "3f6e25cc-05af-4786-fd99-be9e4c07d4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 6s 32ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.76      0.80      2435\n",
            "           1       0.79      0.86      0.82      2565\n",
            "\n",
            "    accuracy                           0.81      5000\n",
            "   macro avg       0.81      0.81      0.81      5000\n",
            "weighted avg       0.81      0.81      0.81      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "import pandas as pd\n",
        "\n",
        "# Mapping labels to sentiment\n",
        "label_map = {0: 'Negative', 1: 'Positive'}\n",
        "\n",
        "# Convert labels to sentiment\n",
        "train_sentiments = [label_map[label] for label in train_labels]\n",
        "test_sentiments = [label_map[label] for label in test_labels]\n",
        "\n",
        "# Create DataFrame to visualize the data\n",
        "train_df = pd.DataFrame({'Sentiment': train_sentiments, 'Review': train_texts})\n",
        "test_df = pd.DataFrame({'Sentiment': test_sentiments, 'Review': test_texts})\n",
        "\n",
        "# Display the first few rows of the datasets\n",
        "print(\"Train data:\")\n",
        "print(train_df.head(10))\n",
        "\n",
        "print(\"\\nTest data:\")\n",
        "print(test_df.head(10))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLfGoDDfCcdm",
        "outputId": "72b36158-25a7-4964-a75e-f7f645fa15a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data:\n",
            "  Sentiment                                             Review\n",
            "0  Positive  Stuning even for the non-gamer: This sound tra...\n",
            "1  Positive  The best soundtrack ever to anything.: I'm rea...\n",
            "2  Positive  Amazing!: This soundtrack is my favorite music...\n",
            "3  Positive  Excellent Soundtrack: I truly like this soundt...\n",
            "4  Positive  Remember, Pull Your Jaw Off The Floor After He...\n",
            "5  Positive  an absolute masterpiece: I am quite sure any o...\n",
            "6  Negative  Buyer beware: This is a self-published book, a...\n",
            "7  Positive  Glorious story: I loved Whisper of the wicked ...\n",
            "8  Positive  A FIVE STAR BOOK: I just finished reading Whis...\n",
            "9  Positive  Whispers of the Wicked Saints: This was a easy...\n",
            "\n",
            "Test data:\n",
            "  Sentiment                                             Review\n",
            "0  Positive  Great CD: My lovely Pat has one of the GREAT v...\n",
            "1  Positive  One of the best game music soundtracks - for a...\n",
            "2  Negative  Batteries died within a year ...: I bought thi...\n",
            "3  Positive  works fine, but Maha Energy is better: Check o...\n",
            "4  Positive  Great for the non-audiophile: Reviewed quite a...\n",
            "5  Negative  DVD Player crapped out after one year: I also ...\n",
            "6  Negative  Incorrect Disc: I love the style of this, but ...\n",
            "7  Negative  DVD menu select problems: I cannot scroll thro...\n",
            "8  Positive  Unique Weird Orientalia from the 1930's: Exoti...\n",
            "9  Negative  Not an \"ultimate guide\": Firstly,I enjoyed the...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate predictions for the test set\n",
        "# Assuming you have already trained your model and obtained predictions\n",
        "# Replace 'predictions' with your actual predictions\n",
        "# Replace 'model' with your actual trained model\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Convert predictions to sentiment labels\n",
        "predicted_sentiments = [label_map[prediction[0]] for prediction in predictions]  # Accessing individual elements of the NumPy array\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(test_sentiments, predicted_sentiments, average='binary', pos_label='Positive')\n",
        "\n",
        "print(\"F1 Score:\", f1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zHGhbRRAl-5",
        "outputId": "2ad28e36-363b-4aca-8ed5-383cd15c67e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 5s 34ms/step\n",
            "F1 Score: 0.8146543234193421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Amazon reviews\n",
        "example_amazon_reviews = [\n",
        "    \"This product exceeded my expectations. Highly recommended!\",\n",
        "    \"The quality of this product is very poor. I regret buying it.\",\n",
        "    \"Great value for the price. Will buy again.\",\n",
        "    \"I received a defective product. Disappointed with the purchase.\",\n",
        "    \"Excellent customer service. They resolved my issue quickly.\",\n",
        "    \"Worst product ever! Do not waste your money.\",\n",
        "    \"Fast shipping and good packaging. Very satisfied.\",\n",
        "    \"Not as described. Misleading product information.\",\n",
        "    \"I love this product! It's exactly what I was looking for.\",\n",
        "    \"Terrible experience with this seller. Avoid at all costs.\"\n",
        "]\n",
        "\n",
        "# Preprocess the example reviews\n",
        "example_amazon_reviews_cleaned = clean_texts(example_amazon_reviews)\n",
        "\n",
        "# Convert text to sequences\n",
        "example_amazon_sequences = tokenizer.texts_to_sequences(example_amazon_reviews_cleaned)\n",
        "\n",
        "# Pad sequences\n",
        "example_amazon_sequences_padded = pad_sequences(example_amazon_sequences, maxlen=maxlen)\n",
        "\n",
        "# Predict sentiment\n",
        "predictions = model.predict(example_amazon_sequences_padded)\n",
        "\n",
        "# Set a dynamic threshold based on validation set metrics during model training\n",
        "# Replace 'optimal_threshold' with the threshold obtained during model training\n",
        "optimal_threshold = 0.5\n",
        "\n",
        "# Convert probabilities to sentiment labels using the optimal threshold\n",
        "sentiment_labels = ['Positive' if pred >= optimal_threshold else 'Negative' for pred in predictions]\n",
        "\n",
        "# Print example reviews and their predicted sentiments\n",
        "for review, sentiment in zip(example_amazon_reviews, sentiment_labels):\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKnsPCKf7jAJ",
        "outputId": "387caf72-9851-47cc-aae5-480157a43474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--100.00%--Done !\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Review: This product exceeded my expectations. Highly recommended!\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: The quality of this product is very poor. I regret buying it.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: Great value for the price. Will buy again.\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: I received a defective product. Disappointed with the purchase.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: Excellent customer service. They resolved my issue quickly.\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: Worst product ever! Do not waste your money.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: Fast shipping and good packaging. Very satisfied.\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: Not as described. Misleading product information.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: I love this product! It's exactly what I was looking for.\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: Terrible experience with this seller. Avoid at all costs.\n",
            "Predicted Sentiment: Negative\n",
            "\n"
          ]
        }
      ]
    }
  ]
}